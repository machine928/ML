# 机器学习课程项目报告

姓名：朱政同	学号：20245227002	方向：视频对象分割

​	本项目报告围绕“家庭电力消耗”问题展开，对家庭电力消耗进行多变量时间序列预测，任务数据集来自UCI Machine Learning Repository公开的“Individual household electric power consumption” 数据集，项目报告分为：问题介绍、模型、结果与分析、讨论和引用五个部分。代码在：https://github.com/machine928/ML.git

## 问题介绍

​	**任务简介**

​	本项目任务为多变量时间序列预测，且根据需要预测的序列长短分为短期预测（90天）和长期预测（365天），因此我们需要根据序列长短分别训练、评估两套模型 (短期预测版本简称为short，长期预测版本简称为long)。本项目共涉及三套模型：LSTM、Transformer和改进模型。

​	**主要实验环境**	

​	python=3.8

​	pytorch=2.1.0

​	cuda=12.1

​	gpu：v100 * 1

​	**数据集处理**

​	1）预处理：由于原始数据集是以分钟为为最小单位划分的，而预测任务则要求以天为单位进行预测，因此我们首先根据提示对数据集进行聚合操作（按天聚合），同时计算得到“sub_metering_remainder”数据列，以月为单位的数据均取当天第一个数据，并且以0填充空值。详见代码：utils/prepare_data.py。

​	2）滑动窗口：滑动窗口的实现过程借鉴了博客[1]，在短期序列中，我们将x和y的窗口大小均设定为90，在长期序列中我们将x的窗口大小设定为90，将y的窗口大小设定为365。两种序列的滑动步长均设置为1。滑动窗口的核心代码：

```
def sliding_window(self, start=0):
    data = self.df
    x, y = [], []

    for _ in range(len(data)):
        in_end = start + self.window_size
        out_end = in_end + self.pred_size

        if out_end < len(data):
            x.append(data.iloc[start:in_end, 1:])
            y.append(data.iloc[in_end:out_end, 1])
        start += self.window_step

    x, y = np.array(x), np.array(y)
    self.x_shape = x.shape
    self.y_shape = y.shape

    return x, y
```

​	3）归一化：为了防止在训练过程中梯度爆炸，我们对数据集进行0-1归一化，且本实验中的MSE和MAE评价指标均在归一化后的数据上得到。

​	有关数据集的更多细节详见代码：dataset/seq_dataset.py

## 模型

### Ada_LSTM(自适应LSTM)介绍

​	在训练LSTM和Transformer的过程中我们发现lstm在短时间预测上略好于Transformer（Transformer在长时间预测上略好于LSTM），且其收敛速度和显存占用远优于Transformer，且LSTM天然适配序列预测任务，综合考虑我们决定将LSTM作为baseline进行改进。接下来我们从动机、改进和方法流程几个方面展开对我们方法的介绍

#### 动机

​	1）过拟合：在训练LSTM的过程中我们发现其在一定轮次的训练之后会产生过拟合的现象（即训练loss逐步降低，而验证集的loss在升高），最后一个训练轮次得到的预测值与真实值对比图如图1所示，可以看到模型的预测结果完全与训练集样本重合，我们判断这是过拟合结果	

<figure style="text-align: center;">
  <img src="D:\project\WorksProject\ML\plot\train_plot_0.png" alt="过拟合演示" width="700">
  <figcaption>图1.过拟合展示</figcaption>
</figure>
​	2）寻找主特征：受到PCA主成分分析的启发，我们推测在多维时序预测任务当中的“x"中的特征应该在重要程度上有所差异，我们在进行预测时应该把关注度分配给更重要的特征。换言之，我们需要学习每个特征的重要程度，以此在每一个时间步的预测过程中动态的判断各个特征的重要性

​	3）LSTM与Transformer长短互补：我们在实验的过程中观察到在短时间序列的预测上LSTM的表现更佳，在长时间序列的预测上Transformer更佳，因此我们在想能否将两种模型的长处结合起来，结合两种模型架构的输出得到更优结果

#### 改进

​	1）数据增强：解决过拟合问题的有效途径之一就是利用数据增强提示训练样本的多样性，以提升模型的鲁棒性，因此我们参考博客[2]中的序列数据增强方法，对我们的训练集以0.5的概率应用随机噪声、数据偏移等增强方式以获取更丰富的样本

​	2）自适应特征选择：我们在LSTM之前增加一个MLP，利用MLP学习x中各个特征的权重，然后将学习到的权重乘以x，将权重调整过的x输入到LSTM进行后续预测

```
weight = mlp(x) 
x = x * weight

out = lstm(x)
```

​	3）集成模型：我们将Transformer的输出和LSTM的输出拼接起来，并利用一个全连接层将拼接后的输出重新映射回原始输出大小，从而得到两种模型的混合输出

```
lstm_out = lstm(x)
transformer_out = transformer(x)
combined = torch.cat(lstm_out, transformer_out)

out = fc(combined)
```

#### 方法流程图

<figure style="text-align: center;">
  <img src="D:\project\WorksProject\ML\plot\流程图.png" alt="过拟合演示" width="700">
  <figcaption>图2.方法流程图</figcaption>
</figure>

### 训练参数

​	我们在这一小节汇报在不同模型的训练超参数设定

#### 基础模型

**LSTM (short)**

| 实验编号 | 训练轮次 | 学习率 | batch size | 损失函数 | Y窗口大小/滑动步长 | 优化器 |
| :------: | :------: | :----: | :--------: | :------: | :----------------: | :----: |
|    1     |   1000   |  1e-4  |     64     |   MSE    |        90/1        |  Adam  |
|    2     |   1000   |  1e-4  |     64     |   MSE    |        90/7        |  Adam  |
|    3     |   1000   |  1e-4  |     32     |   MSE    |       90/30        |  Adam  |
|    4     |   1000   |  1e-4  |     16     |   MSE    |       90/45        |  Adam  |

<p style="text-align: center;">表1 LSTM (short)训练设置</p>

**LSTM (long)**

| 实验编号 | 训练轮次 | 学习率 | batch size | 损失函数 | Y窗口大小/滑动步长 | 优化器 |
| :------: | :------: | :----: | :--------: | :------: | :----------------: | :----: |
|    1     |   1000   |  1e-4  |     64     |   MSE    |       365/1        |  Adam  |
|    2     |   1000   |  1e-4  |     32     |   MSE    |       365/7        |  Adam  |
|    3     |   1000   |  1e-4  |     16     |   MSE    |       365/30       |  Adam  |
|    4     |   1000   |  1e-4  |     16     |   MSE    |       365/90       |  Adam  |

<p style="text-align: center;">表2 LSTM (long)训练设置</p>

**Transformer (short)**

| 实验编号 | 训练轮次 | 学习率 | batch size | 损失函数 | Y窗口大小/滑动步长 | 优化器 |
| :------: | :------: | :----: | :--------: | :------: | :----------------: | :----: |
|    1     |   1000   |  1e-4  |     64     |   MSE    |        90/1        | Adamw  |
|    2     |   1000   |  1e-4  |     64     |   MSE    |        90/7        | Adamw  |
|    3     |   1000   |  1e-4  |     32     |   MSE    |       90/30        | Adamw  |
|    4     |   1000   |  1e-4  |     16     |   MSE    |       90/45        | Adamw  |

<p style="text-align: center;">表3 Transformer (short)训练设置</p>

**Transformer (long)**

| 实验编号 | 训练轮次 | 学习率 | batch size | 损失函数 | Y窗口大小/滑动步长 | 优化器 |
| :------: | :------: | :----: | :--------: | :------: | :----------------: | :----: |
|    1     |   500    |  1e-4  |     16     |   MSE    |       365/1        | Adamw  |
|    2     |   500    |  1e-4  |     16     |   MSE    |       365/7        | Adamw  |
|    3     |   500    |  1e-4  |     16     |   MSE    |       365/30       | Adamw  |
|    4     |   500    |  1e-4  |     16     |   MSE    |       365/90       | Adamw  |

<p style="text-align: center;">表4 Transformer (long)训练设置</p>

#### 改进模型

**Ada_LSTM (short)**

| 实验编号 | 训练轮次 | 学习率 | batch size | 损失函数 | Y窗口大小/滑动步长 | 优化器 |               改进               |
| :------: | :------: | :----: | :--------: | :------: | :----------------: | :----: | :------------------------------: |
|    1     |   1000   |  1e-4  |     64     |   MSE    |        90/1        |  Adam  |             数据增强             |
|    2     |   1000   |  1e-4  |     64     |   MSE    |        90/1        |  Adam  |          自适应特征选择          |
|    3     |   1000   |  1e-4  |     64     |   MSE    |        90/1        |  Adam  |             集成模型             |
|    4     |   1000   |  1e-4  |     64     |   MSE    |        90/1        |  Adam  |     数据增强+自适应特征选择      |
|    5     |   1000   |  1e-4  |     64     |   MSE    |        90/1        |  Adam  | 数据增强+自适应特征选择+集成模型 |

<p style="text-align: center;">表5 Ada_LSTM (short)训练设置</p>

**Ada_LSTM (long)**

| 实验编号 | 训练轮次 | 学习率 | batch size | 损失函数 | Y窗口大小/滑动步长 | 优化器 |               改进               |
| :------: | :------: | :----: | :--------: | :------: | :----------------: | :----: | :------------------------------: |
|    1     |   1000   |  1e-4  |     64     |   MSE    |       365/7        |  Adam  |             数据增强             |
|    2     |   1000   |  1e-4  |     64     |   MSE    |       365/7        |  Adam  |          自适应特征选择          |
|    3     |   1000   |  1e-4  |     64     |   MSE    |       365/7        |  Adam  |             集成模型             |
|    4     |   1000   |  1e-4  |     64     |   MSE    |       365/7        |  Adam  |     数据增强+自适应特征选择      |
|    5     |   1000   |  1e-4  |     64     |   MSE    |       365/7        |  Adam  | 数据增强+自适应特征选择+集成模型 |

<p style="text-align: center;">表6 Ada_LSTM (long)训练设置</p>

## 结果与分析

### 定量结果

​	我们在这一节汇报三种模型的定量结果，并在最后挑选出每种模型的最优结果进行对比

#### 基础模型

**LSTM (short)**

| 实验编号 | MSE (平均) | Std (MSE) | MAE (平均) | Std (MAE) | 实验轮次 (样本量) |
| :------: | :--------: | :-------: | :--------: | :-------: | :---------------: |
|    1     | **0.0300** |  0.0099   |   0.1361   |  0.0237   |        515        |
|    2     |   0.0345   |  0.0132   |   0.1474   |  0.0320   |        515        |
|    3     |   0.0399   |  0.0142   |   0.1573   |  0.0328   |        515        |
|    4     |   0.0409   |  0.0139   |   0.1595   |  0.0308   |        515        |

<p style="text-align: center;">表7 LSTM (short)评估结果</p>

**LSTM (long)**

| 实验编号 | MSE (平均) | Std (MSE) | MAE (平均) | Std (MAE) | 实验轮次 (样本量) |
| :------: | :--------: | :-------: | :--------: | :-------: | :---------------: |
|    1     |   0.0483   |  0.0023   |   0.1785   |  0.0043   |        240        |
|    2     | **0.0400** |  0.0084   |   0.1563   |  0.0191   |        240        |
|    3     |   0.0525   |  0.0123   |   0.1806   |  0.0232   |        240        |
|    4     |   0.1364   |  0.0399   |   0.3127   |  0.0477   |        240        |

<p style="text-align: center;">表8 LSTM (long)评估结果</p>

**Transformer (short)**

| 实验编号 | MSE (平均) | Std (MSE) | MAE (平均) | Std (MAE) | 实验轮次 (样本量) |
| :------: | :--------: | :-------: | :--------: | :-------: | :---------------: |
|    1     | **0.0321** |  0.0211   |   0.1409   |  0.0464   |        515        |
|    2     |   0.0411   |  0.0195   |   0.1615   |  0.0412   |        515        |
|    3     |   0.0399   |  0.0166   |   0.1574   |  0.0394   |        515        |
|    4     |   0.0464   |  0.0245   |   0.1719   |  0.0537   |        515        |

<p style="text-align: center;">表9 Transformer (short)评估结果</p>

**Transformer (long)**

| 实验编号 | MSE (平均) | Std (MSE) | MAE (平均) | Std (MAE) | 实验轮次 (样本量) |
| :------: | :--------: | :-------: | :--------: | :-------: | :---------------: |
|    1     |   0.0418   |  0.0154   |   0.1593   |  0.0334   |        240        |
|    2     | **0.0336** |  0.0029   |   0.1428   |  0.0064   |        240        |
|    3     |   0.0340   |  0.0036   |   0.1441   |  0.0081   |        240        |
|    4     |   0.0337   |  0.0032   |   0.1430   |  0.0071   |        240        |

<p style="text-align: center;">表10 Transformer (long)评估结果</p>

#### 改进模型

**Ada_LSTM (short)**

| 实验编号 | MSE (平均) | Std (MSE) | MAE (平均) | Std (MAE) | 实验轮次 (样本量) |               改进               |
| :------: | :--------: | :-------: | :--------: | :-------: | :---------------: | :------------------------------: |
|    0     |   0.0300   |  0.0099   |   0.1361   |  0.0237   |        515        |             baseline             |
|    1     | **0.0260** |  0.0130   |   0.1234   |  0.0333   |        515        |             数据增强             |
|    2     |   0.0266   |  0.0115   |   0.1265   |  0.0299   |        515        |          自适应特征选择          |
|    3     |   0.0271   |  0.0147   |   0.1281   |  0.0328   |        515        |             集成模型             |
|    4     |   0.0285   |  0.0110   |   0.1324   |  0.0301   |        515        |     数据增强+自适应特征选择      |
|    5     |   0.0299   |  0.0173   |   0.134    |  0.0414   |        515        | 数据增强+自适应特征选择+集成模型 |

<p style="text-align: center;">表11 Ada_LSTM (short)评估结果</p>

**Ada_LSTM (long)**

| 实验编号 | MSE (平均) | Std (MSE) | MAE (平均) | Std (MAE) | 实验轮次 (样本量) |               改进               |
| :------: | :--------: | :-------: | :--------: | :-------: | :---------------: | :------------------------------: |
|    0     |   0.0400   |  0.0084   |   0.1563   |  0.0191   |        240        |             baseline             |
|    1     |   0.0416   |  0.0167   |   0.1585   |  0.0360   |        240        |             数据增强             |
|    2     |   0.0415   |  0.0127   |   0.1588   |  0.0285   |        240        |          自适应特征选择          |
|    3     |   0.0374   |  0.0035   |   0.1521   |  0.0087   |        240        |             集成模型             |
|    4     |   0.0394   |  0.0083   |   0.1554   |  0.0192   |        240        |     数据增强+自适应特征选择      |
|    5     | **0.0372** |  0.0034   |   0.1514   |  0.0089   |        240        | 数据增强+自适应特征选择+集成模型 |

<p style="text-align: center;">表12 Ada_LSTM (long)评估结果</p>

#### 定量结果对比

​	我们在这一小节提供三种模型最优结果的定量对比，每一列的short版本最好结果用<span style="color: red;">**红色粗体**</span>标出，每一列的long版本最好结果用<span style="color: blue;">**蓝色粗体**</span>标出

|       模型名        |                  MSE (平均)                  |                  Std (MSE)                   |                  MAE (平均)                  |                  Std (MAE)                   |
| :-----------------: | :------------------------------------------: | :------------------------------------------: | :------------------------------------------: | :------------------------------------------: |
|    LSTM (short)     |                    0.0300                    | <span style="color: red;">**0.0099**</span>  |                    0.1361                    | <span style="color: red;">**0.0237**</span>  |
|     LSTM (long)     |                    0.0400                    |                    0.0084                    |                    0.1563                    |                    0.0191                    |
| Transformer (short) |                    0.0321                    |                    0.0211                    |                    0.1409                    |                    0.0464                    |
| Transformer (long)  | <span style="color: blue;">**0.0336**</span> | <span style="color: blue;">**0.0029**</span> | <span style="color: blue;">**0.1428**</span> | <span style="color: blue;">**0.0064**</span> |
|  Ada_LSTM (short)   | <span style="color: red;">**0.0260**</span>  |                    0.0130                    | <span style="color: red;">**0.1234**</span>  |                    0.0333                    |
|   Ada_LSTM (long)   |                    0.0372                    |                    0.0034                    |                    0.1514                    |                    0.0089                    |

<p style="text-align: center;">表13 定量结果对比</p>

### 定性结果

​	在这一节我们汇报各个模型的定性实验结果，为了保证实验的公平性，我们挑选每轮实验的第一个样本绘制出的结果进行汇报

#### 基础模型

**LSTM (short)**

<figure style="text-align: center;">
    <img src="D:\project\WorksProject\ML\plot\lstm_short.png" alt="过拟合演示" width="700">
	<figcaption>图3.LSTM (short)结果展示 (a)~(d)依次代表表7实验编号1~4的结果</figcaption>
</figure>

**LSTM (long)**

<figure style="text-align: center;">
    <img src="D:\project\WorksProject\ML\plot\lstm_long.png" alt="过拟合演示" width="700">
	<figcaption>图4.LSTM (long)结果展示 (a)~(d)依次代表表8实验编号1~4的结果</figcaption>
</figure>

**Transformer (short)**

<figure style="text-align: center;">
    <img src="D:\project\WorksProject\ML\plot\transformer_short.png" alt="过拟合演示" width="700">
	<figcaption>图5.Transformer (short)结果展示 (a)~(d)依次代表表9实验编号1~4的结果</figcaption>
</figure>

**Transformer (long)**

<figure style="text-align: center;">
    <img src="D:\project\WorksProject\ML\plot\transformer_long.png" alt="过拟合演示" width="700">
	<figcaption>图6.Transformer (long)结果展示 (a)~(d)依次代表表10实验编号1~4的结果</figcaption>
</figure>

#### 改进模型

**Ada_LSTM (short)**

<figure style="text-align: center;">
    <img src="D:\project\WorksProject\ML\plot\ada_lstm_short.png" alt="过拟合演示" width="700">
	<figcaption>图7.Ada_LSTM (short)结果展示 (a)~(e)依次代表表10实验编号1~5的结果(我们仅在窗口/步长为90/1的训练设置下训练模型)</figcaption>
</figure>

**Ada_LSTM (long)**

<figure style="text-align: center;">
    <img src="D:\project\WorksProject\ML\plot\ada_lstm_long.png" alt="过拟合演示" width="700">
	<figcaption>图8.Ada_LSTM (long)结果展示 (a)~(e)依次代表表10实验编号1~5的结果(我们仅在窗口/步长为365/7的训练设置下训练模型)</figcaption>
</figure>



### 结果分析

​	**对比结果分析** 根据表13中汇报的结果，我们的改进方法在short版本中的性能位列第一，而在long版本中的性能位列第二，证明了我们方法能够有效的结合两种模型结构的优势，在强化LSTM短时序预测上的性能时，还能使其适应不同长度的多变量时序预测问题

​	**消融结果分析** 根据表11中对于short版本的消融实验的结果来看，我们提出的三种改进方法单独采用时都有明显的效果提升，而三种改进措施都加入时效果反而下降，我们推测可能是更负责的模型结构在面对短序列预测时可能会导致过拟合的问题，又或许是Transformer在短时序预测方面的短板影响了最终的结果

​	表12中对于long版本的消融实验结果来看，数据增强和自适应特征选择的改进方案并没有起到促进作用，而集成模型的改进起到了较好的提升效果，证明我们利用两种模型优势互补的想法是有效的

## 讨论

​	在本项目中，我们通过预处理以分钟为划分单位的电力数据集得到以天为划分单位的电力数据集。我们在该数据集上训练了三种模型：LSTM、Transformer和Ada_LSTM，每种模型分为short和long两个版本，并且每种模型都尝试了不同的超参数设置，我们以相同的样本设置测试并对比了这些模型，同时还对我们改进的模型进行了充分的消融实验，最后的实验结果表明我们的改进方法能够在提升LSTM短期时序预测能力的基础上，进一步强化其在长期时序预测中的能力

​	然而受限于数据集样本量，我们依然无法有效解决过拟合的问题，模型训练到一定轮次后依然会出现训练loss下降，验证loss上升的情况，同时我们仅在短期预测上位于三种模型第一，在长期预测上我们的方法仍存在一定的不足



## 引用

[1] [时间序列预测15：Multi-input / Multi-head CNN 实现用电量/发电量预测_multi head cnn-CSDN博客](https://blog.csdn.net/weixin_39653948/article/details/105431099)

[2] [Tsaug:时间序列数据增强库-CSDN博客](https://blog.csdn.net/qq_42820800/article/details/141054027)

[3] [PCA：详解主成分分析_pca主成分分析-CSDN博客](https://blog.csdn.net/wfengzi5/article/details/147075455?ops_request_misc=%7B%22request%5Fid%22%3A%22441d085af8786205d3015df07eade023%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=441d085af8786205d3015df07eade023&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-147075455-null-null.142^v102^pc_search_result_base6&utm_term=pca主成分&spm=1018.2226.3001.4187)

[4] [如何优化LSTM模型的性能：具体实践指南_lstm改进-CSDN博客](https://blog.csdn.net/fenglingguitar/article/details/141758961?ops_request_misc=&request_id=&biz_id=102&utm_term=lstm改进&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-141758961.142^v102^pc_search_result_base6&spm=1018.2226.3001.4187)

[5] Attention is all you need

[6] Long short-term memory

[7] [Transformer完整实现教程_transformer实现-CSDN博客](https://blog.csdn.net/m0_59931331/article/details/146191125?ops_request_misc=%7B%22request%5Fid%22%3A%228dc0e45f0c77daacb67c6460ac599c2b%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=8dc0e45f0c77daacb67c6460ac599c2b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-4-146191125-null-null.142^v102^pc_search_result_base6&utm_term=Transformer的实现&spm=1018.2226.3001.4187)

[8] [pytorch实现LSTM（附code）_pytorch lstm-CSDN博客](https://blog.csdn.net/ting_qifengl/article/details/113039454?ops_request_misc=%7B%22request%5Fid%22%3A%22fa885263867b27968f7a057333b772f6%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=fa885263867b27968f7a057333b772f6&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-113039454-null-null.142^v102^pc_search_result_base6&utm_term=lstm的实现&spm=1018.2226.3001.4187)

